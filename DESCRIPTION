Package: madgrad
Title: 'MADGRAD' Method for Stochastic Optimization
Version: 0.1.0
Authors@R: c(
    person("Daniel", "Falbel", email = "daniel@rstudio.com", role = c("aut", "cre", "cph")),
    person(family = "RStudio", role = c("cph")),
    person(family = "MADGRAD original implementation authors.", role = c("cph"))
    )
Description: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic 
  Optimization algorithm. MADGRAD is a 'best-of-both-worlds' optimizer with the 
  generalization performance of stochastic gradient descent and at least as fast 
  convergence as that of Adam, often faster. A drop-in optim_madgrad() implementation
  is provided based on Defazio et al (2020) <arxiv:2101.11075>.
License: MIT + file LICENSE
Encoding: UTF-8
RoxygenNote: 7.1.1
Imports: torch (>= 0.3.0), rlang
Suggests: testthat (>= 3.0.0)
Config/testthat/edition: 3
NeedsCompilation: no
Packaged: 2021-05-07 14:17:39 UTC; dfalbel
Author: Daniel Falbel [aut, cre, cph],
  RStudio [cph],
  MADGRAD original implementation authors. [cph]
Maintainer: Daniel Falbel <daniel@rstudio.com>
Repository: CRAN
Date/Publication: 2021-05-10 07:30:12 UTC
